{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d6d348",
   "metadata": {
    "id": "27d6d348"
   },
   "source": [
    "# [Transcribing YouTube videos with Whisper in a few lines of code](https://medium.com/nlplanet/transcribing-youtube-videos-with-whisper-in-a-few-lines-of-code-f57f27596a55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4954cd3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21454,
     "status": "ok",
     "timestamp": 1687227686762,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "4954cd3d",
    "outputId": "986fad11-3c0f-4e80-fa7c-95a00a30103d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-yw83hmbj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-yw83hmbj\n",
      "  Resolved https://github.com/openai/whisper.git to commit 248b6cb124225dd263bb9bd32d060b6517e067f8\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
      "Collecting tiktoken==0.3.3 (from openai-whisper==20230314)\n",
      "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.27.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.5)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798075 sha256=658b27006ae1a3710b3b333ab4b30bacf225d7c7c78dd6dde78ea50bad469deb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-23rsyxct/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: tiktoken, openai-whisper\n",
      "Successfully installed openai-whisper-20230314 tiktoken-0.3.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pytube\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytube\n",
      "Successfully installed pytube-15.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4696880d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4686,
     "status": "ok",
     "timestamp": 1687227691444,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "4696880d"
   },
   "outputs": [],
   "source": [
    "import pytube as pt\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc6019a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1687227692729,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "9fc6019a",
    "outputId": "3dff09b7-a991-43f1-f7a4-d0d82d063803"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/audio_english.mp3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download mp3 from youtube video (Two Minute Papers)\n",
    "yt = pt.YouTube(\"https://www.youtube.com/watch?v=dd1kN_myNDs\")\n",
    "stream = yt.streams.filter(only_audio=True)[0]\n",
    "stream.download(filename=\"audio_english.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a7df0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50051,
     "status": "ok",
     "timestamp": 1687227742774,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "47a7df0d",
    "outputId": "82f298ed-763e-4647-fa90-c1789fc98c80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [00:30<00:00, 50.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91174248",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26882,
     "status": "ok",
     "timestamp": 1687227769650,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "91174248",
    "outputId": "54433b60-be34-4a0f-8ccb-2e68fff653f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are many AI techniques that are able to look at a still image and identify objects, textures, human poses, and object parts in them really well. However, in the age of the internet, we have videos everywhere, so an important question would be how we could do the same for these animations. One of the key ideas in this paper is that the frames of these videos are not completely independent and they share a lot of information, so after we make our initial predictions on what is where exactly, these predictions from the previous frame can almost always be reused with a little modification. Not only that, but here you can see with these results that it can also deal with momentary occlusions and is ready to track objects that rotate over time. A key part of this method is that one, it looks back and forth in these videos to update these labels, and second, it learns in a self-supervised manner, which means that all it is given is just a little more than data and was never given a nice dataset with explicit labels of these regions and object parts that it could learn from. You can see in this comparison table that this is not the only method that works for videos, the paper contains ample comparisons against other methods and comes out ahead unsupervised methods, and on this task, it can even get quite close to supervised methods. The supervised methods are the ones that have access to these cushy labeled datasets and therefore should come out way ahead, but they don't which sounds like witchcraft, considering that this technique is learning on its own. However, all this greatness comes with limitations. One of the bigger ones is that even though it does extremely well, it also plateaus, meaning that we don't see a great deal of improvement if we add more training data. Now whether this is because it is doing nearly as well as it is humanly or computerly possible, or because a more general problem formulation is still possible remains a question. I hope we find out soon. Thanks for watching and for your generous support, and I'll see you next time!\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"audio_english.mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f74b0eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1687228187870,
     "user": {
      "displayName": "michael Chen",
      "userId": "15713746087224002478"
     },
     "user_tz": -480
    },
    "id": "3f74b0eb",
    "outputId": "677e9369-acf5-4f4e-d7d2-230ce5c90ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Ther\n",
      "e are many AI techniques that are able to look at a still image and identify obj\n",
      "ects, textures, human poses, and object parts in them really well. However, in t\n",
      "he age of the internet, we have videos everywhere, so an important question woul\n",
      "d be how we could do the same for these animations. One of the key ideas in this\n",
      " paper is that the frames of these videos are not completely independent and the\n",
      "y share a lot of information, so after we make our initial predictions on what i\n",
      "s where exactly, these predictions from the previous frame can almost always be \n",
      "reused with a little modification. Not only that, but here you can see with thes\n",
      "e results that it can also deal with momentary occlusions and is ready to track \n",
      "objects that rotate over time. A key part of this method is that one, it looks b\n",
      "ack and forth in these videos to update these labels, and second, it learns in a\n",
      " self-supervised manner, which means that all it is given is just a little more \n",
      "than data and was never given a nice dataset with explicit labels of these regio\n",
      "ns and object parts that it could learn from. You can see in this comparison tab\n",
      "le that this is not the only method that works for videos, the paper contains am\n",
      "ple comparisons against other methods and comes out ahead unsupervised methods, \n",
      "and on this task, it can even get quite close to supervised methods. The supervi\n",
      "sed methods are the ones that have access to these cushy labeled datasets and th\n",
      "erefore should come out way ahead, but they don't which sounds like witchcraft, \n",
      "considering that this technique is learning on its own. However, all this greatn\n",
      "ess comes with limitations. One of the bigger ones is that even though it does e\n",
      "xtremely well, it also plateaus, meaning that we don't see a great deal of impro\n",
      "vement if we add more training data. Now whether this is because it is doing nea\n",
      "rly as well as it is humanly or computerly possible, or because a more general p\n",
      "roblem formulation is still possible remains a question. I hope we find out soon\n",
      ". Thanks for watching and for your generous support, and I'll see you next time!\n"
     ]
    }
   ],
   "source": [
    "text = result[\"text\"]\n",
    "while len(text)>0:\n",
    "    print(text[:80])\n",
    "    text = text[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r6XNA8S-ouV4",
   "metadata": {
    "id": "r6XNA8S-ouV4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
