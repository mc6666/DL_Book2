{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 木棒台車(CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隨機行動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入木棒台車(CartPole)遊戲\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 參數設定\n",
    "no = 50 # 設定比賽回合數\n",
    "all_rewards=[] # 每回合總報酬\n",
    "all_steps=[] # 每回合總步數\n",
    "total_rewards = 0\n",
    "total_steps=0\n",
    "\n",
    "# 重置\n",
    "observation, info = env.reset()\n",
    "while no > 0:   # 執行 50 比賽回合數\n",
    "    # 隨機行動\n",
    "    action = env.action_space.sample() \n",
    "    total_steps+=1\n",
    "\n",
    "    # 觸動下一步\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    # 累計報酬\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 比賽回合結束，重置\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "        all_rewards.append(total_rewards)\n",
    "        all_steps.append(total_steps)\n",
    "        total_rewards = 0\n",
    "        total_steps=0\n",
    "        no-=1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t報酬\t結果\n",
      "0\t16.0\tLoss\n",
      "1\t61.0\tLoss\n",
      "2\t13.0\tLoss\n",
      "3\t19.0\tLoss\n",
      "4\t14.0\tLoss\n",
      "5\t21.0\tLoss\n",
      "6\t32.0\tLoss\n",
      "7\t9.0\tLoss\n",
      "8\t18.0\tLoss\n",
      "9\t18.0\tLoss\n",
      "10\t21.0\tLoss\n",
      "11\t36.0\tLoss\n",
      "12\t19.0\tLoss\n",
      "13\t25.0\tLoss\n",
      "14\t40.0\tLoss\n",
      "15\t18.0\tLoss\n",
      "16\t25.0\tLoss\n",
      "17\t10.0\tLoss\n",
      "18\t36.0\tLoss\n",
      "19\t19.0\tLoss\n",
      "20\t17.0\tLoss\n",
      "21\t52.0\tLoss\n",
      "22\t51.0\tLoss\n",
      "23\t10.0\tLoss\n",
      "24\t11.0\tLoss\n",
      "25\t22.0\tLoss\n",
      "26\t19.0\tLoss\n",
      "27\t27.0\tLoss\n",
      "28\t19.0\tLoss\n",
      "29\t15.0\tLoss\n",
      "30\t41.0\tLoss\n",
      "31\t11.0\tLoss\n",
      "32\t20.0\tLoss\n",
      "33\t24.0\tLoss\n",
      "34\t24.0\tLoss\n",
      "35\t13.0\tLoss\n",
      "36\t14.0\tLoss\n",
      "37\t18.0\tLoss\n",
      "38\t14.0\tLoss\n",
      "39\t39.0\tLoss\n",
      "40\t53.0\tLoss\n",
      "41\t19.0\tLoss\n",
      "42\t19.0\tLoss\n",
      "43\t10.0\tLoss\n",
      "44\t35.0\tLoss\n",
      "45\t33.0\tLoss\n",
      "46\t18.0\tLoss\n",
      "47\t40.0\tLoss\n",
      "48\t13.0\tLoss\n",
      "49\t19.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 顯示執行結果\n",
    "print('回合\\t報酬\\t結果')\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 傳統解法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# 參數設定\n",
    "left, right = 0, 1  # 台車行進方向\n",
    "max_angle = 8       # 偏右8度以上，就往右前進，偏左也是同樣處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # 初始化\n",
    "    def __init__(self):\n",
    "        self.direction = left\n",
    "        self.last_direction=right\n",
    "        \n",
    "    # 自訂策略\n",
    "    def act(self, observation):\n",
    "        # 台車位置、台車速度、平衡桿角度、平衡桿速度\n",
    "        cart_position, cart_velocity, pole_angle, pole_velocity = observation\n",
    "        \n",
    "        '''\n",
    "        行動策略：\n",
    "        1. 設定每次行動採一左一右，盡量不離中心點。\n",
    "        2. 平衡桿角度偏右8度以上，就往右前進，直到角度偏右小於8度。\n",
    "        3. 反之，偏左也是同樣處理。\n",
    "        '''\n",
    "        if pole_angle < math.radians(max_angle) and \\\n",
    "            pole_angle > math.radians(-max_angle):\n",
    "            self.direction = (self.last_direction + 1) % 2\n",
    "        elif pole_angle >= math.radians(max_angle):\n",
    "            self.direction = right\n",
    "        else:\n",
    "            self.direction = left\n",
    "\n",
    "        self.last_direction = self.direction\n",
    "        \n",
    "        return self.direction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置\n",
    "observation, info  = env.reset()\n",
    "all_rewards=[] # 每回合總報酬\n",
    "all_steps=[] # 每回合總步數\n",
    "total_rewards = 0\n",
    "total_steps=0\n",
    "no = 50        # 比賽回合數\n",
    "\n",
    "agent = Agent()\n",
    "while no > 0:   # 執行 50 比賽回合數\n",
    "    # 行動\n",
    "    action = agent.act(observation) #env.action_space.sample()\n",
    "    total_steps+=1\n",
    "\n",
    "    # 觸動下一步\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    # 累計報酬\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 比賽回合結束，重置\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "        all_rewards.append(total_rewards)\n",
    "        total_rewards = 0\n",
    "        all_steps.append(total_steps)\n",
    "        total_steps = 0\n",
    "        no-=1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t報酬\t結果\n",
      "0\t116.0\tLoss\n",
      "1\t46.0\tLoss\n",
      "2\t105.0\tLoss\n",
      "3\t90.0\tLoss\n",
      "4\t132.0\tLoss\n",
      "5\t111.0\tLoss\n",
      "6\t118.0\tLoss\n",
      "7\t139.0\tLoss\n",
      "8\t94.0\tLoss\n",
      "9\t108.0\tLoss\n",
      "10\t102.0\tLoss\n",
      "11\t84.0\tLoss\n",
      "12\t108.0\tLoss\n",
      "13\t50.0\tLoss\n",
      "14\t98.0\tLoss\n",
      "15\t66.0\tLoss\n",
      "16\t43.0\tLoss\n",
      "17\t117.0\tLoss\n",
      "18\t94.0\tLoss\n",
      "19\t80.0\tLoss\n",
      "20\t94.0\tLoss\n",
      "21\t86.0\tLoss\n",
      "22\t69.0\tLoss\n",
      "23\t79.0\tLoss\n",
      "24\t104.0\tLoss\n",
      "25\t76.0\tLoss\n",
      "26\t113.0\tLoss\n",
      "27\t121.0\tLoss\n",
      "28\t92.0\tLoss\n",
      "29\t122.0\tLoss\n",
      "30\t49.0\tLoss\n",
      "31\t96.0\tLoss\n",
      "32\t106.0\tLoss\n",
      "33\t63.0\tLoss\n",
      "34\t118.0\tLoss\n",
      "35\t101.0\tLoss\n",
      "36\t189.0\tLoss\n",
      "37\t163.0\tLoss\n",
      "38\t88.0\tLoss\n",
      "39\t121.0\tLoss\n",
      "40\t73.0\tLoss\n",
      "41\t145.0\tLoss\n",
      "42\t94.0\tLoss\n",
      "43\t66.0\tLoss\n",
      "44\t59.0\tLoss\n",
      "45\t83.0\tLoss\n",
      "46\t77.0\tLoss\n",
      "47\t49.0\tLoss\n",
      "48\t80.0\tLoss\n",
      "49\t135.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 顯示執行結果\n",
    "print('回合\\t報酬\\t結果')\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下程式來自：[『From Scratch_ AI Balancing Act in 50 Lines of Python』](https://towardsdatascience.com/from-scratch-ai-balancing-act-in-50-lines-of-python-7ea67ef717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy):\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    score = 0\n",
    "    observations = []\n",
    "    \n",
    "    # 訓練5000步\n",
    "    for _ in range(5000):\n",
    "        observations += [observation.tolist()] # 記錄歷次狀態\n",
    "        \n",
    "        if done: # 回合是否勝負已分\n",
    "            break\n",
    "                \n",
    "        # 行動策略選擇\n",
    "        outcome = np.dot(policy, observation)\n",
    "        action = 1 if outcome > 0 else 0\n",
    "        \n",
    "        # 觸發下一步\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        score += reward\n",
    "\n",
    "    return score, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 236.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 訓練 10 回合\n",
    "max = (0, [], [])\n",
    "for _ in range(10):\n",
    "    policy = np.random.rand(1,4) # 產生4個隨機變數 [0, 1)\n",
    "    score, observations = play(env, policy) # 開始玩\n",
    "    \n",
    "    if score > max[0]: # 取最大分數\n",
    "        max = (score, observations, policy)\n",
    "\n",
    "print('Max Score', max[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 500.0\n"
     ]
    }
   ],
   "source": [
    "# 最終版本\n",
    "max = (0, [], [])\n",
    "\n",
    "for _ in range(100): # 訓練 100 回合    \n",
    "    policy = np.random.rand(1,4) - 0.5  # 改為 [-0.5, 0.5]\n",
    "    score, observations = play(env, policy)\n",
    "    \n",
    "    if score > max[0]:  # 取最大分數\n",
    "        max = (score, observations, policy)\n",
    "        \n",
    "print('Max Score', max[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以最大分數的policy進行實驗，驗證最佳策略是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06344476, -0.10146005,  0.33464762,  0.2047181 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得最佳策略\n",
    "policy = max[2]\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以最佳策略取代隨機policy，進行 10 回合驗證    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  500.0\n",
      "Score:  112.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  154.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10): \n",
    "    score, observations = play(env, policy)\n",
    "    print('Score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
